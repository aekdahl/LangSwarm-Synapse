"use strict";(self.webpackChunkdocusaurus_docs=self.webpackChunkdocusaurus_docs||[]).push([[12],{4809:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>t,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"Tutorials/branching","title":"Branching with a LangChain","description":"This tutorial demonstrates how to use LangSwarm\'s LLMBranching with LangChain in a complete pipeline. By leveraging LangSwarm\u2019s LangChain-compatible tools, we can generate diverse outputs from multiple agents and process those outputs with LangChain for further analysis.","source":"@site/docs/Tutorials/branching.md","sourceDirName":"Tutorials","slug":"/Tutorials/branching","permalink":"/LangSwarm/Tutorials/branching","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":101,"frontMatter":{"title":"Branching with a LangChain","sidebar_position":101},"sidebar":"defaultSidebar","previous":{"title":"Aggregation with LangChain","permalink":"/LangSwarm/Tutorials/aggregation"},"next":{"title":"Consensus with LangChain","permalink":"/LangSwarm/Tutorials/consensus"}}');var a=i(4848),r=i(8453);const t={title:"Branching with a LangChain",sidebar_position:101},o="Branching with a LangChain",l={},p=[{value:"<strong>Prerequisites</strong>",id:"prerequisites",level:2},{value:"<strong>Overview</strong>",id:"overview",level:2},{value:"<strong>Full Code</strong>",id:"full-code",level:2},{value:"<strong>Code Walkthrough</strong>",id:"code-walkthrough",level:2},{value:"<strong>Step 1: Pre-processing Chain</strong>",id:"step-1-pre-processing-chain",level:3},{value:"<strong>Step 2: Branching Chain</strong>",id:"step-2-branching-chain",level:3},{value:"<strong>Step 3: Post-processing Chain</strong>",id:"step-3-post-processing-chain",level:3},{value:"<strong>Step 4: Build and Run the Pipeline</strong>",id:"step-4-build-and-run-the-pipeline",level:3},{value:"<strong>Expected Output</strong>",id:"expected-output",level:2},{value:"<strong>Why This is Powerful</strong>",id:"why-this-is-powerful",level:2}];function h(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"branching-with-a-langchain",children:"Branching with a LangChain"})}),"\n",(0,a.jsxs)(e.p,{children:["This tutorial demonstrates how to use ",(0,a.jsx)(e.strong,{children:"LangSwarm's LLMBranching"})," with LangChain in a complete pipeline. By leveraging LangSwarm\u2019s LangChain-compatible tools, we can generate diverse outputs from multiple agents and process those outputs with LangChain for further analysis."]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:(0,a.jsx)(e.strong,{children:"Prerequisites"})}),"\n",(0,a.jsx)(e.p,{children:"Ensure you have the necessary libraries installed:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"pip install langswarm langchain langchain-openai transformers\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:(0,a.jsx)(e.strong,{children:"Overview"})}),"\n",(0,a.jsx)(e.p,{children:"We will:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["Use LangChain\u2019s ",(0,a.jsx)(e.code,{children:"LLMChain"})," for pre-processing input."]}),"\n",(0,a.jsxs)(e.li,{children:["Utilize LangSwarm\u2019s ",(0,a.jsx)(e.code,{children:"LLMBranchingChain"})," to generate diverse responses from multiple agents."]}),"\n",(0,a.jsx)(e.li,{children:"Integrate a post-processing step with LangChain to analyze and refine the outputs."}),"\n",(0,a.jsxs)(e.li,{children:["Combine all steps into a LangChain ",(0,a.jsx)(e.code,{children:"SequentialChain"})," pipeline for a unified workflow."]}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"full-code",children:(0,a.jsx)(e.strong,{children:"Full Code"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'"""\nLangSwarm + LangChain Pipeline Tutorial: Branching Swarm\n\nThis tutorial demonstrates how to use LangSwarm\'s LangChain-compatible tools\nto include `LLMBranching` directly in a LangChain pipeline with pre- and post-processing steps.\n\nKey Features:\n- Use LangSwarm\u2019s built-in LangChain-compatible branching chain.\n- Pre-processing and post-processing steps in the pipeline.\n- Generate diverse outputs and analyze them in a LangChain workflow.\n"""\n\n# Import necessary libraries\nfrom langswarm.chains import LLMBranchingChain  # LangSwarm\'s LangChain-compatible branching chain\nfrom langchain.prompts import PromptTemplate  # LangChain prompt templates\nfrom langchain.chains import SequentialChain, LLMChain  # LangChain pipeline framework\nfrom langchain.llms import OpenAI  # LangChain OpenAI LLM\n\n# Step 1: Pre-processing Chain\n"""\nThe pre-processing chain ensures the input is clean and formatted before branching.\n"""\npre_prompt = PromptTemplate(\n    input_variables=["raw_input"],\n    template="Clean and standardize the following input for generating ideas:\\n\\n{raw_input}"\n)\npre_chain = LLMChain(llm=OpenAI(model="gpt-3.5-turbo"), prompt=pre_prompt)\n\n# Step 2: Branching Chain\n"""\nUse LangSwarm\'s LangChain-compatible `LLMBranchingChain` to generate diverse responses\nfrom multiple agents.\n"""\nbranching_chain = LLMBranchingChain(\n    agents=[\n        {"model": "gpt-4", "api_key": "your_openai_api_key"},\n        {"model": "gpt-3.5-turbo", "api_key": "your_openai_api_key"},\n        {"model": "gpt-3.5-turbo-instruct", "api_key": "your_openai_api_key"},\n    ]\n)\n\n# Step 3: Post-processing Chain\n"""\nThe post-processing chain refines the diverse outputs and generates a final analysis.\n"""\npost_prompt = PromptTemplate(\n    input_variables=["branching_responses"],\n    template=(\n        "Given the following diverse responses:\\n\\n"\n        "{branching_responses}\\n\\n"\n        "Analyze these ideas and suggest the top three actionable recommendations."\n    )\n)\npost_chain = LLMChain(llm=OpenAI(model="gpt-3.5-turbo"), prompt=post_prompt)\n\n# Step 4: Build the LangChain Pipeline\n"""\nCombine the pre-processing, branching, and post-processing steps into a LangChain pipeline.\n"""\npipeline = SequentialChain(\n    chains=[pre_chain, branching_chain, post_chain],\n    input_variables=["raw_input"],\n    output_variables=["branching_responses", "recommendations"],\n)\n\n# Step 5: Run the Pipeline\n"""\nRun the pipeline on a query, demonstrating the complete multi-agent workflow.\n"""\nquery = "Propose a marketing strategy for an eco-friendly product."\nresults = pipeline.run({"raw_input": query})\n\n# Display the results\nprint("\\nBranching Responses:", results["branching_responses"])\nprint("\\nTop Recommendations:", results["recommendations"])\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"code-walkthrough",children:(0,a.jsx)(e.strong,{children:"Code Walkthrough"})}),"\n",(0,a.jsx)(e.h3,{id:"step-1-pre-processing-chain",children:(0,a.jsx)(e.strong,{children:"Step 1: Pre-processing Chain"})}),"\n",(0,a.jsxs)(e.p,{children:["The pre-processing chain ensures that the input is formatted and cleaned before being sent to the branching swarm. It uses LangChain\u2019s ",(0,a.jsx)(e.code,{children:"LLMChain"}),"."]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'pre_prompt = PromptTemplate(\n    input_variables=["raw_input"],\n    template="Clean and standardize the following input for generating ideas:\\n\\n{raw_input}"\n)\npre_chain = LLMChain(llm=OpenAI(model="gpt-3.5-turbo"), prompt=pre_prompt)\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h3,{id:"step-2-branching-chain",children:(0,a.jsx)(e.strong,{children:"Step 2: Branching Chain"})}),"\n",(0,a.jsxs)(e.p,{children:["LangSwarm\u2019s ",(0,a.jsx)(e.code,{children:"LLMBranchingChain"})," generates diverse responses by querying multiple agents. Each agent explores the query from a different perspective, creating a set of varied outputs."]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'branching_chain = LLMBranchingChain(\n    agents=[\n        {"model": "gpt-4", "api_key": "your_openai_api_key"},\n        {"model": "gpt-3.5-turbo", "api_key": "your_openai_api_key"},\n        {"model": "gpt-3.5-turbo-instruct", "api_key": "your_openai_api_key"},\n    ]\n)\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h3,{id:"step-3-post-processing-chain",children:(0,a.jsx)(e.strong,{children:"Step 3: Post-processing Chain"})}),"\n",(0,a.jsx)(e.p,{children:"The post-processing chain refines and analyzes the diverse responses, providing actionable recommendations based on the branching outputs."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'post_prompt = PromptTemplate(\n    input_variables=["branching_responses"],\n    template=(\n        "Given the following diverse responses:\\n\\n"\n        "{branching_responses}\\n\\n"\n        "Analyze these ideas and suggest the top three actionable recommendations."\n    )\n)\npost_chain = LLMChain(llm=OpenAI(model="gpt-3.5-turbo"), prompt=post_prompt)\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h3,{id:"step-4-build-and-run-the-pipeline",children:(0,a.jsx)(e.strong,{children:"Step 4: Build and Run the Pipeline"})}),"\n",(0,a.jsxs)(e.p,{children:["The ",(0,a.jsx)(e.code,{children:"SequentialChain"})," combines all steps into a single LangChain pipeline. This modular approach makes it easy to extend or modify the workflow as needed."]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'pipeline = SequentialChain(\n    chains=[pre_chain, branching_chain, post_chain],\n    input_variables=["raw_input"],\n    output_variables=["branching_responses", "recommendations"],\n)\n\nquery = "Propose a marketing strategy for an eco-friendly product."\nresults = pipeline.run({"raw_input": query})\n\nprint("\\nBranching Responses:", results["branching_responses"])\nprint("\\nTop Recommendations:", results["recommendations"])\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"expected-output",children:(0,a.jsx)(e.strong,{children:"Expected Output"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-plaintext",children:"Branching Responses:\n1. Promote the product via social media influencers who focus on sustainability.\n2. Offer discounts for customers who recycle old products.\n3. Collaborate with eco-friendly brands to create bundled deals.\n\nTop Recommendations:\n1. Create a partnership program with eco-friendly influencers.\n2. Launch a rewards program for customers who demonstrate sustainable practices.\n3. Host community events to raise awareness about eco-friendly products.\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"why-this-is-powerful",children:(0,a.jsx)(e.strong,{children:"Why This is Powerful"})}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Exploration of Ideas"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Generates diverse outputs from multiple agents, ensuring a range of perspectives and solutions."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Seamless Integration"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Combines LangSwarm\u2019s branching capabilities with LangChain for advanced analysis."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Modular Design"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Easily extendable pipeline for handling additional pre- or post-processing steps."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["With LangSwarm\u2019s ",(0,a.jsx)(e.code,{children:"LLMBranchingChain"}),", you can efficiently generate and refine diverse ideas, making it an excellent tool for brainstorming, problem-solving, and creative workflows."]})]})}function c(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(h,{...n})}):h(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(n){const e=s.useContext(r);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);